# 🧠 Neural Networks from Scratch with Python & NumPy

This repository contains my implementation of neural networks **from scratch** using only **Python** and **NumPy**, based on the excellent [@Neural Networks from Scratch](https://youtube.com/playlist?list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3&si=zHnjIBpu71T853N5) playlist by [@sentdex](https://github.com/Sentdex/NNfSiX/tree/master/Python).

---

## 📚 What You’ll Find Here

This repo includes:

- ✅ Forward propagation logic
- ✅ Backpropagation and gradient calculations
- ✅ Fully connected layers (`Dense` layers)
- ✅ Activation functions: ReLU, Softmax
- ✅ Loss functions: Categorical Crossentropy, MSE
- ✅ Optimizers: SGD, Adam
- ✅ Complete training loop
- ✅ Toy dataset training examples (spiral, classification tasks)
- ✅ Modular and well-commented code

---

## 🧠 Core Concepts Learned

- Neural network architecture
- Matrix/vector operations with NumPy
- Derivatives and gradients
- Building loss and activation functions manually
- Optimization from scratch
- Debugging and improving training performance

---

## 🚀 Getting Started

### 🔧 Prerequisites

- Python 3.7+
- NumPy

```bash
pip install numpy
````

## 🤝 Acknowledgements

Huge thanks to [@sentdex](https://github.com/Sentdex/NNfSiX/tree/master/Python) for the detailed and insightful tutorial series. This project helped me build intuition behind how neural networks actually work at the core level.

